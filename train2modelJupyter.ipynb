{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec52d879",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (338025 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Epoch 1: 100%|██████████| 10563/10563 [15:26<00:00, 11.40batch/s, Loss=8.87]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 average loss: 8.87\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 10563/10563 [15:29<00:00, 11.36batch/s, Loss=8.87]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 average loss: 8.87\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 10563/10563 [15:28<00:00, 11.38batch/s, Loss=8.87]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 average loss: 8.87\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('finetuned_gpt2\\\\tokenizer_config.json',\n",
       " 'finetuned_gpt2\\\\special_tokens_map.json',\n",
       " 'finetuned_gpt2\\\\vocab.json',\n",
       " 'finetuned_gpt2\\\\merges.txt',\n",
       " 'finetuned_gpt2\\\\added_tokens.json')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, AdamW, get_linear_schedule_with_warmup\n",
    "from tqdm import tqdm\n",
    "\n",
    "def batchify(sequence, batch_size):\n",
    "    num_batches = len(sequence) // batch_size\n",
    "    return [sequence[i*batch_size:(i+1)*batch_size] for i in range(num_batches)]\n",
    "\n",
    "# Set up the device (GPU or CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2').to(device)\n",
    "\n",
    "with open('input.txt', 'r') as file:\n",
    "    text = file.read()\n",
    "    \n",
    "tokens = tokenizer.encode(text)\n",
    "\n",
    "epochs = 3\n",
    "batch_size = 32\n",
    "learning_rate = 10e-10\n",
    "warmup_steps = 3000\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "total_steps = len(tokens) // (batch_size * epochs)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps, num_training_steps=total_steps)\n",
    "\n",
    "token_batches = batchify(tokens, batch_size)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    progress_bar = tqdm(token_batches, desc=f'Epoch {epoch+1}', unit='batch')\n",
    "\n",
    "    for token_batch in progress_bar:\n",
    "        input_ids = torch.tensor(token_batch).unsqueeze(0).to(device)\n",
    "        labels = input_ids.clone().detach()\n",
    "        labels[0, :-1] = input_ids[0, 1:]\n",
    "        labels[0, -1] = input_ids[0, 0]\n",
    "        outputs = model(input_ids=input_ids, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        progress_bar.set_postfix({'Loss': f'{epoch_loss / (progress_bar.n + 1):.2f}'})\n",
    "    \n",
    "    print(f'Epoch {epoch+1} average loss: {epoch_loss / len(token_batches):.2f}')\n",
    "\n",
    "model.save_pretrained('finetuned_gpt2')\n",
    "tokenizer.save_pretrained('finetuned_gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cc4fd7dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "# Load the fine-tuned GPT-2 model and tokenizer\n",
    "model = GPT2LMHeadModel.from_pretrained(\"finetuned_gpt2\")\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"finetuned_gpt2\")\n",
    "\n",
    "# Set the maximum sequence length\n",
    "MAX_LENGTH = 1024\n",
    "\n",
    "# Define the API endpoint for text generation\n",
    "@app.post(\"/generate_text/\")\n",
    "async def generate_text(prompt: str):\n",
    "    # Encode the prompt using the tokenizer\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "\n",
    "    # Generate text with the model\n",
    "    output = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        max_length=MAX_LENGTH,\n",
    "        temperature=0.7,\n",
    "        do_sample=True,\n",
    "        top_k=50,\n",
    "        top_p=0.95,\n",
    "        repetition_penalty=1.0,\n",
    "        num_return_sequences=1,\n",
    "    )\n",
    "\n",
    "    # Decode the generated text and return it\n",
    "    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return {\"generated_text\": generated_text}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
